@inproceedings{1389727,
  title = {Design and Use Paradigms for {{Gazebo}}, an Open-Source Multi-Robot Simulator},
  booktitle = {2004 {{IEEE}}/{{RSJ}} International Conference on Intelligent Robots and Systems ({{IROS}}) ({{IEEE}} Cat. {{No}}.{{04CH37566}})},
  author = {Koenig, N. and Howard, A.},
  year = {2004},
  volume = {3},
  pages = {2149-2154 vol.3},
  doi = {10.1109/IROS.2004.1389727},
  file = {/home/ming/snap/zotero-snap/common/Zotero/storage/IWIMKPSZ/Koenig 和 Howard - 2004 - Design and use paradigms for gazebo, an open-sourc.pdf}
}

@inproceedings{5267989,
  title = {Trajectory Tracking for Nonholonomic Wheeled Mobile Robots Based on an Improved Sliding Mode Control Method},
  booktitle = {2009 {{ISECS}} International Colloquium on Computing, Communication, Control, and Management},
  author = {Li, Yandong and Zhu, Ling and Wang, Zongyi and Liu, Tao},
  year = {2009},
  month = aug,
  volume = {2},
  pages = {55--58},
  issn = {2154-963X},
  doi = {10.1109/CCCM.2009.5267989},
  abstract = {In this paper, an improved sliding mode algorithm with global asymptotic stability is presented for tracking control of nonholonomic mobile robot. Centroid offset parameters are used in the design of the controller, which can drive the position error to zero quickly in the premise of guaranteeing stability of the whole system. The Kinematics mathematical model and the position and posture error model of the mobile robot are presented. Switch function of variable structure control is designed based on the backstepping method. Comparative experiments are given and simulation results show that the proposed control method is effective and has better control performances.},
  keywords = {Asymptotic stability,Backstepping,backstepping technique,Control systems,Error correction,improved sliding mode control,kinematic model,Kinematics,Mathematical model,mobile robot,Mobile robots,Sliding mode control,Switches,Trajectory,trajectory tracking}
}

@inproceedings{5518600,
  title = {Research on Trajectory Tracking Control of Nonholonomic Wheeled Mobile Service Robots},
  booktitle = {International Technology and Innovation Conference 2009 ({{ITIC}} 2009)},
  author = {Chang, Jiang and Meng, Qingxin},
  year = {2009},
  month = oct,
  pages = {1--5},
  doi = {10.1049/cp.2009.1523},
  abstract = {Nonholonomic wheeled mobile service robot's posture error model denoted by cartesian coordinates in local coordinates is established. A novel nonlinear state feedback trajectory tracking control law is proposed, which causes closed-loop system state space euqation of robot to have isolated euqilibrium state at origin. Through analysing the local uniform asymptotical stability at origin and the instability of isolated boundary equilibrium state at non-origin under the proposed trajectory tracking control law, the scale of control parameters is confirmed. By Lyapunov candidate function method, this paper concludes that the elosed-loop system is globally uniformly asymptotically stable at origin. Simulation results show the effectiveness of the proposed control law.},
  keywords = {Lyapunov candidate function method,nonholonomic,nonlinear state feedback,trajectory tracking control,wheeled mobile robots}
}

@article{5699388,
  title = {Adaptive Sliding Mode Control for Attitude Stabilization with Actuator Saturation},
  author = {Zhu, Zheng and Xia, Yuanqing and Fu, Mengyin},
  year = {2011},
  month = oct,
  journal = {IEEE Transactions on Industrial Electronics},
  volume = {58},
  number = {10},
  pages = {4898--4907},
  issn = {1557-9948},
  doi = {10.1109/TIE.2011.2107719},
  abstract = {The problem of attitude stabilization for a spacecraft system which is nonlinear in dynamics with inertia uncertainty and external disturbance is investigated in this paper. An adaptive law is applied to estimate the disturbances, where a sliding mode controller is designed to force the state variables of the closed-loop system to converge to the origin. Then, the spacecraft system subjected to control constraints is further considered, and another adaptive sliding mode control law is designed to achieve the attitude stabilization. No prior knowledge of inertia moment is required for both of the proposed adaptive control laws, which implies that the designed control schemes can be applied in spacecraft systems with a large parametric uncertainty existing in inertial matrix or even in unknown inertial matrix. Also, simulation results are presented to illustrate the effectiveness of the control strategies.},
  keywords = {Actuators,Adaptive control,Attitude control,attitude stabilization,control saturation,Sliding mode control,sliding mode control (SMC),Space vehicles,Uncertainty,Upper bound}
}

@inproceedings{6901011,
  title = {Formation Stabilization of Nonholonomic Robots Using Nonlinear Model Predictive Control},
  booktitle = {2014 {{IEEE}} 27th Canadian Conference on Electrical and Computer Engineering ({{CCECE}})},
  author = {Mehrez, Mohamed W. and Mann, George K. I. and Gosine, Raymond G.},
  year = {2014},
  month = may,
  pages = {1--6},
  issn = {0840-7789},
  doi = {10.1109/CCECE.2014.6901011},
  abstract = {This paper compares two approaches of multi-robots' formation stabilization using nonlinear model predictive control (NMPC), namely, centralized and distributed predictive controls. Centralized NMPC has been highlighted in the literature to lead superior performance; however, it has been remarked with high computational power requirements which limit its application to practical formation stabilization problems. Nonetheless, in this paper, the use of a recently developed toolkit implementing fast NMPC algorithms rendered this problem tractable. The performance of the two control approaches are compared in a series of numerical simulations. The results demonstrated that the centralized controller has a better performance compared with its distributed counterpart. Furthermore, it showed real-time requirements satisfaction.},
  keywords = {Collision avoidance,Mobile robots,Optimization,Robot kinematics,Trajectory,Vectors}
}

@inproceedings{7320765,
  title = {Model Predictive Trajectory Tracking Control for Hydraulic Excavator on Digging Operation},
  booktitle = {2015 {{IEEE}} Conference on Control Applications ({{CCA}})},
  author = {Tomatsu, Takumi and Nonaka, Kenichiro and Sekiguchi, Kazuma and Suzuki, Katsumasa},
  year = {2015},
  month = sep,
  pages = {1136--1141},
  issn = {1085-1992},
  doi = {10.1109/CCA.2015.7320765},
  abstract = {In order to increase work efficiency, alleviating burden of operators is important. An autonomous hydraulic excavator is expected to improve it. In this paper, an automatic control of a digging operation for the hydraulic excavator is studied. We propose a method for the trajectory tracking control using model predictive control (MPC) which incorporates servo mechanism. MPC can optimize motion and avoids rapid change of velocity using constraints. However, it is difficult to cope with unknown reaction forces caused by contacting with underground objects. Servo mechanism suppresses the disturbance by the integration of the tracking error. However, the error may be accumulated in the integration. Hence, the trajectory tracking may result in rapid response when the objects are removed. By combining MPC and servo mechanism, we can expect that servo mechanism works against the disturbance and the tracking performance is improved. We show effectiveness of the proposed method through simulations under the presence of the disturbance.},
  file = {/home/ming/snap/zotero-snap/common/Zotero/storage/PI4DSPP2/7320765.html}
}

@article{7323849,
  title = {Robust Control of a Vehicle Steer-by-Wire System Using Adaptive Sliding Mode},
  author = {Sun, Zhe and Zheng, Jinchuan and Man, Zhihong and Wang, Hai},
  year = {2016},
  month = apr,
  journal = {IEEE Transactions on Industrial Electronics},
  volume = {63},
  number = {4},
  pages = {2251--2262},
  issn = {1557-9948},
  doi = {10.1109/TIE.2015.2499246},
  abstract = {This paper presents an adaptive sliding-mode (ASM) control methodology for a vehicle steer-by-wire (SbW) system. First, the SbW system is modeled as a second-order system from the steering motor input voltage to the front-wheel steering angle. For simplicity, the self-aligning torque and friction arising from the tire-to-ground contact are regarded as external disturbance acting on the SbW system. Next, an ASM controller is designed for the SbW system, which can not only cope with the parametric uncertainties in the plant model but also estimate the coefficient of the self-aligning torque effectively. The stability of the ASM control system is proved in the sense of Lyapunov and the guidelines for selecting the control parameters are given. Finally, experiments are carried out for steering control to respectively follow a slalom path and a circular path under various road conditions. It is shown that the proposed ASM controller can achieve stronger robustness against various road conditions leading to significantly smaller tracking errors in comparison with a conventional sliding-mode controller and a linear H{$\infty$} controller.},
  keywords = {Adaptive sliding mode,Adaptive sliding mode (ASM),Mathematical model,Roads,robust control,Robustness,self-aligning torque,selfaligning torque,Torque,Uncertainty,vehicle steer-by-wire (SbW) system,vehicle steer-by-wire system,Vehicles,Wheels}
}

@book{8585411,
  title = {An Introduction to Deep Reinforcement Learning},
  author = {{Fran{\c c}ois-Lavet}, Vincent and Henderson, Peter and Islam, Riashat and Bellemare, Marc G. and Pineau, Joelle},
  year = {2018},
  file = {/home/ming/snap/zotero-snap/common/Zotero/storage/TJ2PLFS4/François-Lavet 等 - 2018 - An introduction to deep reinforcement learning}
}

@article{amerModellingControlStrategies2017,
  title = {Modelling and {{Control Strategies}} in {{Path Tracking Control}} for {{Autonomous Ground Vehicles}}: {{A Review}} of {{State}} of the {{Art}} and {{Challenges}}},
  shorttitle = {Modelling and {{Control Strategies}} in {{Path Tracking Control}} for {{Autonomous Ground Vehicles}}},
  author = {Amer, Noor Hafizah and Zamzuri, Hairi and Hudha, Khisbullah and Kadir, Zulkiffli Abdul},
  year = {2017},
  month = may,
  journal = {Journal of Intelligent \& Robotic Systems},
  volume = {86},
  number = {2},
  pages = {225--254},
  issn = {0921-0296, 1573-0409},
  doi = {10.1007/s10846-016-0442-0},
  urldate = {2023-01-09},
  langid = {english},
  file = {/home/ming/snap/zotero-snap/common/Zotero/storage/FBHNPCR4/Amer 等 - 2017 - Modelling and Control Strategies in Path Tracking .pdf}
}

@inproceedings{amidiIntegratedMobileRobot1991a,
  title = {Integrated Mobile Robot Control},
  booktitle = {Mobile {{Robots V}}},
  author = {Amidi, Omead and Thorpe, Chuck E},
  year = {1991},
  volume = {1388},
  pages = {504--523},
  publisher = {{SPIE}},
  doi = {10.1117/12.25494},
  file = {/home/ming/snap/zotero-snap/common/Zotero/storage/YXA8TV2L/Amidi_Thorpe_1991_Integrated mobile robot control.pdf}
}

@inproceedings{ammarRobustPathTracking2020,
  title = {Robust {{Path Tracking}} of {{Mobile Robot Using Fractional Order PID Controller}}},
  booktitle = {The {{International Conference}} on {{Advanced Machine Learning Technologies}} and {{Applications}} ({{AMLTA2019}})},
  author = {Ammar, Hossam Hassan and Azar, Ahmad Taher},
  editor = {Hassanien, Aboul Ella and Azar, Ahmad Taher and Gaber, Tarek and Bhatnagar, Roheet and F. Tolba, Mohamed},
  year = {2020},
  series = {Advances in {{Intelligent Systems}} and {{Computing}}},
  pages = {370--381},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-14118-9_37},
  abstract = {This paper represents the control of the Pioneer-3 Mobile Robot as a complex non-linear system which provides an object for research nonlinear system kinematics and dynamics analysis. In this paper, the system modeling and simulation is divided into two main parts. The first part is the modeling and simulation using MATLAB and the second part is the whole mechanical design and its characteristics as a function of the motor speed and the torque depending on the system using Virtual Robot Environment Program (V-REP). The study uses Proportional\textendash Integral\textendash Derivative (PID) and Fractional Order PID (FOPID) controllers to obtain a robust controller for the system. The linear velocity loop controls the robot wheels speeds using the motor speed feedback signal from the encoder. The angular velocity control loop keeps the robot always in the accepted angle boundary using a six-degree of freedom gyroscope and accelerometer as a feedback signal. A state space model is obtained considering some assumptions and simplifications. This paper also studies and compares the results of two controllers PID and FOPID controllers from analysis perspectives with different optimization methods. The results demonstrated that the FOPID controller is superior in performance to the traditional PID controller.},
  isbn = {978-3-030-14118-9},
  langid = {english},
  keywords = {Controller tuning and optimization,Fractional Order PID,Mobile robot modeling and simulation,Path tracking,PID,Pioneer-3 Mobile Robot,V-REP},
  file = {/home/ming/snap/zotero-snap/common/Zotero/storage/DXD4WTHL/Ammar_Azar_2020_Robust Path Tracking of Mobile Robot Using Fractional Order PID Controller.pdf}
}

@article{b.ravikiranDeepReinforcementLearning2021,
  title = {Deep Reinforcement Learning for Autonomous Driving: {{A}} Survey},
  shorttitle = {Deep Reinforcement Learning for Autonomous Driving},
  author = {{B. Ravi Kiran} and Sobh, Ibrahim and Talpaert, Victor and Mannion, Patrick and Al Sallab, Ahmad A. and Yogamani, Senthil and P{\'e}rez, Patrick},
  year = {2021},
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  publisher = {{IEEE}},
  file = {/home/ming/snap/zotero-snap/common/Zotero/storage/GUNBTF2T/Kiran 等 - 2021 - Deep reinforcement learning for autonomous driving.pdf;/home/ming/snap/zotero-snap/common/Zotero/storage/BJSXC7MC/9351818.html}
}

@misc{bealApplicationsMPCVehicle2011,
  title = {Applications of {{MPC}} to Vehicle Dynamics for Active Safety and Stability. {{PhD}}, {{Department}} of {{Mechanical Engineering}}},
  author = {Beal, C. E.},
  year = {2011},
  publisher = {{Stanford University}}
}

@book{bellmanAppliedDynamicProgramming2015,
  title = {Applied Dynamic Programming},
  author = {Bellman, Richard E. and Dreyfus, Stuart E.},
  year = {2015},
  volume = {2050},
  publisher = {{Princeton university press}},
  file = {/home/ming/snap/zotero-snap/common/Zotero/storage/NEB6788Q/books.html}
}

@article{bellmanDynamicProgrammingApplied1962,
  title = {Dynamic Programming Applied to Control Processes Governed by General Functional Equations},
  author = {Bellman, Richard and Kalaba, Robert},
  year = {1962},
  journal = {Proceedings of the National Academy of Sciences},
  volume = {48},
  number = {10},
  pages = {1735--1737},
  publisher = {{National Acad Sciences}},
  doi = {10.1073/pnas.48.10.1735},
  file = {/home/ming/snap/zotero-snap/common/Zotero/storage/LKX3R43A/Bellman_Kalaba_1962_Dynamic programming applied to control processes governed by general functional.pdf;/home/ming/snap/zotero-snap/common/Zotero/storage/5EUUZLDF/pnas.48.10.html}
}

@article{bellmanMarkovianDecisionProcess1957,
  title = {A {{Markovian}} Decision Process},
  author = {Bellman, Richard},
  year = {1957},
  journal = {Journal of mathematics and mechanics},
  pages = {679--684},
  publisher = {{JSTOR}},
  keywords = {⛔ No DOI found},
  file = {/home/ming/snap/zotero-snap/common/Zotero/storage/795EEKZB/24900506.html}
}

@article{bojarskiEndEndLearning2016,
  title = {End to End Learning for Self-Driving Cars},
  author = {Bojarski, Mariusz and Del Testa, Davide and Dworakowski, Daniel and Firner, Bernhard and Flepp, Beat and Goyal, Prasoon and Jackel, Lawrence D. and Monfort, Mathew and Muller, Urs and Zhang, Jiakai},
  year = {2016},
  journal = {arXiv preprint arXiv:1604.07316},
  eprint = {1604.07316},
  archiveprefix = {arxiv},
  file = {/home/ming/snap/zotero-snap/common/Zotero/storage/H8GCFUGU/Bojarski 等 - 2016 - End to end learning for self-driving cars.pdf;/home/ming/snap/zotero-snap/common/Zotero/storage/2FJDK8QN/1604.html}
}

@inproceedings{chenAttentionbasedHierarchicalDeep2019,
  title = {Attention-Based Hierarchical Deep Reinforcement Learning for Lane Change Behaviors in Autonomous Driving},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}}},
  author = {Chen, Yilun and Dong, Chiyu and Palanisamy, Praveen and Mudalige, Priyantha and Muelling, Katharina and Dolan, John M.},
  year = {2019},
  pages = {0--0},
  keywords = {⛔ No DOI found},
  file = {/home/ming/snap/zotero-snap/common/Zotero/storage/2ZFCU84A/Chen et al_2019_Attention-based hierarchical deep reinforcement learning for lane change.pdf;/home/ming/snap/zotero-snap/common/Zotero/storage/GCFV5KLR/Chen_Attention-Based_Hierarchical_Deep_Reinforcement_Learning_for_Lane_Change_Behaviors_in_CVPR.html}
}

@article{chenInterpretableEndtoEndUrban2022,
  title = {Interpretable {{End-to-End Urban Autonomous Driving With Latent Deep Reinforcement Learning}}},
  author = {Chen, Jianyu and Li, Shengbo Eben and Tomizuka, Masayoshi},
  year = {2022},
  month = jun,
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  volume = {23},
  number = {6},
  pages = {5068--5078},
  issn = {1558-0016},
  doi = {10.1109/TITS.2020.3046646},
  abstract = {Unlike popular modularized framework, end-to-end autonomous driving seeks to solve the perception, decision and control problems in an integrated way, which can be more adapting to new scenarios and easier to generalize at scale. However, existing end-to-end approaches are often lack of interpretability, and can only deal with simple driving tasks like lane keeping. In this article, we propose an interpretable deep reinforcement learning method for end-to-end autonomous driving, which is able to handle complex urban scenarios. A sequential latent environment model is introduced and learned jointly with the reinforcement learning process. With this latent model, a semantic birdeye mask can be generated, which is enforced to connect with certain intermediate properties in today's modularized framework for the purpose of explaining the behaviors of learned policy. The latent space also significantly reduces the sample complexity of reinforcement learning. Comparison tests in a realistic driving simulator show that the performance of our method in urban scenarios with crowded surrounding vehicles dominates many baselines including DQN, DDPG, TD3 and SAC. Moreover, through masked outputs, the learned model is able to provide a better explanation of how the car reasons about the driving environment.},
  keywords = {Autonomous automobiles,Autonomous driving,Autonomous vehicles,deep reinforcement learning,Graphical models,interpretability,probabilistic graphical model,Probabilistic logic,Reinforcement learning,Roads,Task analysis},
  file = {/home/ming/snap/zotero-snap/common/Zotero/storage/7EJ6RS76/Chen 等 - 2022 - Interpretable End-to-End Urban Autonomous Driving .pdf;/home/ming/snap/zotero-snap/common/Zotero/storage/6TUWU83P/9346000.html}
}

@article{chenMPCbasedPathTracking2020a,
  title = {{{MPC-based}} Path Tracking with {{PID}} Speed Control for High-Speed Autonomous Vehicles Considering Time-Optimal Travel},
  author = {Chen, Shu-ping and Xiong, Guang-ming and Chen, Hui-yan and Negrut, Dan},
  year = {2020},
  month = dec,
  journal = {Journal of Central South University},
  volume = {27},
  doi = {10.1007/s11771-020-4561-1},
  abstract = {In order to track the desired path as fast as possible, a novel autonomous vehicle path tracking based on model predictive control (MPC) and PID speed control was proposed for high-speed automated vehicles considering the constraints of vehicle physical limits, in which a forward-backward integration scheme was introduced to generate a time-optimal speed profile subject to the tire-road friction limit. Moreover, this scheme was further extended along one moving prediction window. In the MPC controller, the prediction model was an 8-degree-of-freedom (DOF) vehicle model, while the plant was a 14-DOF vehicle model. For lateral control, a sequence of optimal wheel steering angles was generated from the MPC controller; for longitudinal control, the total wheel torque was generated from the PID speed controller embedded in the MPC framework. The proposed controller was implemented in MATLAB considering arbitrary curves of continuously varying curvature as the reference trajectory. The simulation test results show that the tracking errors are small for vehicle lateral and longitudinal positions and the tracking performances for trajectory and speed are good using the proposed controller. Additionally, the case of extended implementation in one moving prediction window requires shorter travel time than the case implemented along the entire path.},
  file = {/home/ming/snap/zotero-snap/common/Zotero/storage/RQWKHRZX/Chen et al_2020_MPC-based path tracking with PID speed control for high-speed autonomous.pdf}
}

@article{dasilvaSurveyTransferLearning2019,
  title = {A Survey on Transfer Learning for Multiagent Reinforcement Learning Systems},
  author = {Da Silva, Felipe Leno and Costa, Anna Helena Reali},
  year = {2019},
  journal = {Journal of Artificial Intelligence Research},
  volume = {64},
  pages = {645--703},
  file = {/home/ming/snap/zotero-snap/common/Zotero/storage/IDQAKKK6/Da Silva 和 Costa - 2019 - A survey on transfer learning for multiagent reinf.pdf;/home/ming/snap/zotero-snap/common/Zotero/storage/RRHKAV8X/11396.html}
}

@inproceedings{duanPathTrackingControl2013,
  title = {A Path Tracking Control Algorithm with Speed Adjustment for Intelligent Vehicle},
  booktitle = {2013 {{IEEE International Conference}} on {{Robotics}} and {{Biomimetics}} ({{ROBIO}})},
  author = {Duan, Jianmin and Yao, Junqin and Liu, Dan and Liu, Guanyu},
  year = {2013},
  pages = {2397--2402},
  publisher = {{IEEE}},
  doi = {10.1109/ROBIO.2013.6739829},
  file = {/home/ming/snap/zotero-snap/common/Zotero/storage/CRG92X3A/6739829.html}
}

@inproceedings{fujimoto2018addressing,
  title = {Addressing Function Approximation Error in Actor-Critic Methods},
  booktitle = {International Conference on Machine Learning},
  author = {Fujimoto, Scott and Hoof, Herke and Meger, David},
  year = {2018},
  pages = {1587--1596},
  organization = {{PMLR}},
  keywords = {⛔ No DOI found},
  file = {/home/ming/snap/zotero-snap/common/Zotero/storage/Y7MM2QED/Fujimoto 等 - 2018 - Addressing function approximation error in actor-c.pdf;/home/ming/snap/zotero-snap/common/Zotero/storage/Z2EG585N/fujimoto18a.html}
}

@inproceedings{guDeepReinforcementLearning2017,
  title = {Deep Reinforcement Learning for Robotic Manipulation with Asynchronous Off-Policy Updates},
  booktitle = {2017 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Gu, Shixiang and Holly, Ethan and Lillicrap, Timothy and Levine, Sergey},
  year = {2017},
  month = may,
  pages = {3389--3396},
  doi = {10.1109/ICRA.2017.7989385},
  abstract = {Reinforcement learning holds the promise of enabling autonomous robots to learn large repertoires of behavioral skills with minimal human intervention. However, robotic applications of reinforcement learning often compromise the autonomy of the learning process in favor of achieving training times that are practical for real physical systems. This typically involves introducing hand-engineered policy representations and human-supplied demonstrations. Deep reinforcement learning alleviates this limitation by training general-purpose neural network policies, but applications of direct deep reinforcement learning algorithms have so far been restricted to simulated settings and relatively simple tasks, due to their apparent high sample complexity. In this paper, we demonstrate that a recent deep reinforcement learning algorithm based on off-policy training of deep Q-functions can scale to complex 3D manipulation tasks and can learn deep neural network policies efficiently enough to train on real physical robots. We demonstrate that the training times can be further reduced by parallelizing the algorithm across multiple robots which pool their policy updates asynchronously. Our experimental evaluation shows that our method can learn a variety of 3D manipulation skills in simulation and a complex door opening skill on real robots without any prior demonstrations or manually designed representations.},
  keywords = {Heuristic algorithms,Instruction sets,Learning (artificial intelligence),Neural networks,Robots,Safety,Training},
  file = {/home/ming/snap/zotero-snap/common/Zotero/storage/HZC3KMP7/Gu 等 - 2017 - Deep reinforcement learning for robotic manipulati.pdf;/home/ming/snap/zotero-snap/common/Zotero/storage/TDGTP36W/7989385.html}
}

@article{heLaneFollowingMethod2021,
  title = {Lane {{Following Method Based}} on {{Improved DDPG Algorithm}}},
  author = {He, Rui and Lv, Haipeng and Zhang, Sumin and Zhang, Dong and Zhang, Hang},
  year = {2021},
  month = jan,
  journal = {Sensors},
  volume = {21},
  number = {14},
  pages = {4827},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {1424-8220},
  doi = {10.3390/s21144827},
  urldate = {2023-01-15},
  abstract = {In an autonomous vehicle, the lane following algorithm is an important component, which is a basic function of autonomous driving. However, the existing lane following system has a few shortcomings: first, the control method it adopts requires an accurate system model, and different vehicles have different parameters, which needs a lot of parameter calibration work. The second is that it may fail on road sections where the lateral acceleration requirements of vehicles are large, such as large curves. Third, its decision-making system is defined based on rules, which has disadvantages: it is difficult to formulate; human subjective factors cannot guarantee objectivity; coverage is difficult to guarantee. In recent years, the deep deterministic policy gradient (DDPG) algorithm has been widely used in the field of autonomous driving due to its strong nonlinear fitting ability and generalization performance. However, the DDPG algorithm has overestimated state action values and large cumulative errors, low training efficiency and other issues. Therefore, this paper improves the DDPG algorithm based on the double critic networks and priority experience replay mechanism. Then this paper proposes a lane following method based on this algorithm. Experiment shows that the algorithm can achieve excellent following results under various road conditions.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {autonomous driving,deep deterministic policy gradient,deep reinforcement learning,lane following},
  file = {/home/ming/snap/zotero-snap/common/Zotero/storage/7C6GTRJB/He 等 - 2021 - Lane Following Method Based on Improved DDPG Algor.pdf}
}

@article{hesselRainbowCombiningImprovements2018,
  title = {Rainbow: {{Combining Improvements}} in {{Deep Reinforcement Learning}}},
  shorttitle = {Rainbow},
  author = {Hessel, Matteo and Modayil, Joseph and Van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
  year = {2018},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {32},
  number = {1},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v32i1.11796},
  urldate = {2023-01-31},
  abstract = {The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance.},
  langid = {english},
  file = {/home/ming/snap/zotero-snap/common/Zotero/storage/39LTWBXH/Hessel 等 - 2018 - Rainbow Combining Improvements in Deep Reinforcem.pdf}
}

@inproceedings{hilleliDeepReinforcementLearning2018,
  title = {Toward Deep Reinforcement Learning without a Simulator: {{An}} Autonomous Steering Example},
  shorttitle = {Toward Deep Reinforcement Learning without a Simulator},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Hilleli, Bar and {El-Yaniv}, Ran},
  year = {2018},
  volume = {32},
  file = {/home/ming/snap/zotero-snap/common/Zotero/storage/HPARL5MI/Hilleli 和 El-Yaniv - 2018 - Toward deep reinforcement learning without a simul.pdf;/home/ming/snap/zotero-snap/common/Zotero/storage/GSDL24B7/11490.html}
}

@inproceedings{huangEndtoEndAutonomousDriving2019,
  title = {End-to-{{End Autonomous Driving Decision Based}} on {{Deep Reinforcement Learning}}},
  booktitle = {2019 5th {{International Conference}} on {{Control}}, {{Automation}} and {{Robotics}} ({{ICCAR}})},
  author = {Huang, Zhiqing and Zhang, Ji and Tian, Rui and Zhang, Yanxin},
  year = {2019},
  month = apr,
  pages = {658--662},
  issn = {2251-2446},
  doi = {10.1109/ICCAR.2019.8813431},
  abstract = {End-to-end autonomous driving decision-making is a popular research field in autonomous driving. In this paper, we propose an end-to-end decision-making model based on DDPG deep reinforcement learning. Firstly, we establish an end-to-end decision-making model to map driving state (such as tangential angle of vehicle, velocity of vehicle, distance of road) to driving action (steer, accelerate, brake) continuously. Next we train and valid our agent in different scenarios on TORCS platform. The results show that DDPG algorithms can achieve end-to-end autonomous driving decisions. Finally, we visualize the agent by analyzing which state contributes to decision.},
  keywords = {autonomous driving,end-to-end decision,reinforcement learning,TORCS},
  file = {/home/ming/snap/zotero-snap/common/Zotero/storage/QBZ5LC45/8813431.html}
}

@inproceedings{huangSelfAttentionDDPGMultiBeam2022,
  title = {Self-{{Attention DDPG}} for {{Multi-Beam Combining}} in {{mmWave MIMO Systems}}},
  booktitle = {2022 {{IEEE}} 33rd {{Annual International Symposium}} on {{Personal}}, {{Indoor}} and {{Mobile Radio Communications}} ({{PIMRC}})},
  author = {Huang, Yingzhi and Zhang, Zhaoyang and Yang, Zhaohui and Yang, Qianqian},
  year = {2022},
  month = sep,
  pages = {843--848},
  issn = {2166-9589},
  doi = {10.1109/PIMRC54779.2022.9977701},
  abstract = {In this paper, we aim at an efficient multi-beam combining design with only requiring receive power measurements for a millimeter-wave (mmWave) multi-input multi-output (MIMO) communication system. A spectrum efficiency maximization problem is formulated with both beam selection and power constraints. To solve this problem, a reinforcement learning (RL)-based multi-beam combining algorithm is proposed. In particular, a self-attention deep deterministic policy gradient (DDPG) scheme is used to adaptively learn the serving beam sets and the corresponding combining weights without any channel state information (CSI). Moreover, the transformer is integrated into the DDPG to precisely capture the signal directions and relevant strengths. Experimental results show the effectiveness of the proposed learning structure in terms of system achievable rate, convergence, and network robustness.},
  keywords = {Deep deterministic policy gradient (DDPG),Millimeter wave communication,Millimeter wave measurements,MIMO communication,multi-beam combining,Power measurement,Reinforcement learning,Robustness,self-attention,Transformers},
  file = {/home/ming/snap/zotero-snap/common/Zotero/storage/ZUCVQG7K/Huang 等 - 2022 - Self-Attention DDPG for Multi-Beam Combining in mm.pdf;/home/ming/snap/zotero-snap/common/Zotero/storage/JEECWTDA/9977701.html}
}

@misc{JiYuShenDuQiangHuaXueXiDeGaoSuGongLuHuanDaoGenZongKongZhiMoXingZhongGuoZhiWang,
  title = {基于深度强化学习的高速公路换道跟踪控制模型 - 中国知网},
  urldate = {2023-03-09},
  howpublished = {http://kns-cnki-net-s.vpn.ysu.edu.cn:8118/kcms2/article/abstract?v=3uoqIhG8C44YLTlOAiTRKibYlV5Vjs7ioT0BO4yQ4m\_mOgeS2ml3UFbA2suq0VdejqfhrjM7Ov18fg3-olKsPdEk\_A396Vfv\&uniplatform=NZKPT},
  file = {/home/ming/snap/zotero-snap/common/Zotero/storage/ZWHX79AI/abstract.html}
}

@article{josefDeepReinforcementLearning2020,
  title = {Deep Reinforcement Learning for Safe Local Planning of a Ground Vehicle in Unknown Rough Terrain},
  author = {Josef, Shirel and Degani, Amir},
  year = {2020},
  journal = {IEEE Robotics and Automation Letters},
  volume = {5},
  number = {4},
  pages = {6748--6755},
  publisher = {{IEEE}},
  doi = {10.1109/LRA.2020.3011912},
  file = {/home/ming/snap/zotero-snap/common/Zotero/storage/AWCIUT8K/Josef_Degani_2020_Deep reinforcement learning for safe local planning of a ground vehicle in.pdf;/home/ming/snap/zotero-snap/common/Zotero/storage/KNZCXJ95/9149720.html}
}

@article{koberReinforcementLearningRobotics2013,
  title = {Reinforcement Learning in Robotics: {{A}} Survey},
  shorttitle = {Reinforcement Learning in Robotics},
  author = {Kober, Jens and Bagnell, J. Andrew and Peters, Jan},
  year = {2013},
  journal = {The International Journal of Robotics Research},
  volume = {32},
  number = {11},
  pages = {1238--1274},
  publisher = {{SAGE Publications Sage UK: London, England}},
  file = {/home/ming/snap/zotero-snap/common/Zotero/storage/8HHRKMUD/Kober 等 - 2013 - Reinforcement learning in robotics A survey.pdf;/home/ming/snap/zotero-snap/common/Zotero/storage/QWQLKER2/0278364913495721.html}
}

@article{kondaActorcriticAlgorithms1999,
  title = {Actor-Critic Algorithms},
  author = {Konda, Vijay and Tsitsiklis, John},
  year = {1999},
  journal = {Advances in neural information processing systems},
  volume = {12},
  file = {/home/ming/snap/zotero-snap/common/Zotero/storage/N7ZVHLBP/Konda 和 Tsitsiklis - 1999 - Actor-critic algorithms.pdf;/home/ming/snap/zotero-snap/common/Zotero/storage/RI6DZ248/6449f44a102fde848669bdd9eb6b76fa-Abstract.html}
}

@article{kuuttiSurveyDeepLearning2021,
  title = {A {{Survey}} of {{Deep Learning Applications}} to {{Autonomous Vehicle Control}}},
  author = {Kuutti, Sampo and Bowden, Richard and Jin, Yaochu and Barber, Phil and Fallah, Saber},
  year = {2021},
  month = feb,
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  volume = {22},
  number = {2},
  pages = {712--733},
  issn = {1558-0016},
  doi = {10.1109/TITS.2019.2962338},
  abstract = {Designing a controller for autonomous vehicles capable of providing adequate performance in all driving scenarios is challenging due to the highly complex environment and inability to test the system in the wide variety of scenarios which it may encounter after deployment. However, deep learning methods have shown great promise in not only providing excellent performance for complex and non-linear control problems, but also in generalising previously learned rules to new scenarios. For these reasons, the use of deep learning for vehicle control is becoming increasingly popular. Although important advancements have been achieved in this field, these works have not been fully summarised. This paper surveys a wide range of research works reported in the literature which aim to control a vehicle through deep learning methods. Although there exists overlap between control and perception, the focus of this paper is on vehicle control, rather than the wider perception problem which includes tasks such as semantic segmentation and object detection. The paper identifies the strengths and limitations of available deep learning methods through comparative analysis and discusses the research challenges in terms of computation, architecture selection, goal specification, generalisation, verification and validation, as well as safety. Overall, this survey brings timely and topical information to a rapidly evolving field relevant to intelligent transportation systems.},
  keywords = {advanced driver assistance,autonomous vehicles,Autonomous vehicles,computer vision,Deep learning,intelligent control,Machine learning,neural networks,Neural networks,Reinforcement learning,Sensors,Task analysis,Training},
  file = {/home/ming/snap/zotero-snap/common/Zotero/storage/Z3J56GA5/Kuutti et al_2021_A Survey of Deep Learning Applications to Autonomous Vehicle Control.pdf;/home/ming/snap/zotero-snap/common/Zotero/storage/VH8HYZLU/8951131.html}
}

@article{leeNewLaneFollowing2019,
  title = {A New Lane Following Method Based on Deep Learning for Automated Vehicles Using Surround View Images},
  author = {Lee, Minho and Han, Kyung Yeop and Yu, Jihun and Lee, Young-Sup},
  year = {2019},
  month = sep,
  journal = {Journal of Ambient Intelligence and Humanized Computing},
  issn = {1868-5137, 1868-5145},
  doi = {10.1007/s12652-019-01496-8},
  urldate = {2023-01-15},
  langid = {english}
}

@misc{lillicrapContinuousControlDeep2019,
  title = {Continuous Control with Deep Reinforcement Learning},
  author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  year = {2019},
  month = jul,
  number = {arXiv:1509.02971},
  eprint = {arXiv:1509.02971},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1509.02971},
  urldate = {2023-01-31},
  abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/ming/snap/zotero-snap/common/Zotero/storage/KK9FUV9I/Lillicrap 等 - 2019 - Continuous control with deep reinforcement learnin.pdf;/home/ming/snap/zotero-snap/common/Zotero/storage/D649592Q/1509.html}
}

@article{liuReinforcementTrackingEffectiveTrajectory2022,
  title = {Reinforcement-{{Tracking}}: {{An Effective Trajectory Tracking}} and {{Navigation Method}} for {{Autonomous Urban Driving}}},
  shorttitle = {Reinforcement-{{Tracking}}},
  author = {Liu, Meng and Zhao, Fei and Yin, Jialun and Niu, Jianwei and Liu, Yu},
  year = {2022},
  month = jul,
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  volume = {23},
  number = {7},
  pages = {6991--7007},
  issn = {1558-0016},
  doi = {10.1109/TITS.2021.3066366},
  abstract = {In order to improve trajectory tracking accuracy, a reinforcement learning method was employed to address the trajectory tracking task in autonomous driving. There are many conveniences and advantages in theories, methods and tools utilizing reinforcement learning to solve trajectory tracking control problems. To teach an intelligent agent effective driving skills, the structured road information in the urban environment was extracted to generate an accurate reference trajectory. Then real-world scenarios were modeled to build a simulation environment for training. To effectively train the intelligent driving agent, Imitation Learning was firstly employed to teach the agent primary driving skills. Afterwards, Reinforcement Learning was adopted to optimize the agent's driving policy. After the intelligent driving agent was well trained in the simulator, the tracking experiments were conducted in the simulator and the real-world scenarios. The proposed method was compared with base-line methods of geometric tracking, optimization-based tracking and learning-based tracking. The experimental results demonstrated that Reinforcement-Tracking can achieve accurate trajectory tracking performance and even exceed the accuracy of most baseline methods.},
  keywords = {autonomous driving,Autonomous vehicles,DDPG,Navigation,Reinforcement learning,Roads,Training,Trajectory,trajectory tracking,Trajectory tracking},
  file = {/home/ming/snap/zotero-snap/common/Zotero/storage/7SS5JQ4T/Liu 等 - 2022 - Reinforcement-Tracking An Effective Trajectory Tr.pdf;/home/ming/snap/zotero-snap/common/Zotero/storage/WSMH7JSC/references.html}
}

@article{lovejoySurveyAlgorithmicMethods1991,
  title = {A Survey of Algorithmic Methods for Partially Observed {{Markov}} Decision Processes},
  author = {Lovejoy, William S.},
  year = {1991},
  journal = {Annals of Operations Research},
  volume = {28},
  number = {1},
  pages = {47--65},
  publisher = {{Springer}},
  doi = {10.1007/BF02055574},
  file = {/home/ming/snap/zotero-snap/common/Zotero/storage/MYGE2IDC/BF02055574.html}
}

@article{mnihAsynchronousMethodsDeep,
  title = {Asynchronous {{Methods}} for {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Badia, Adri{\`a} Puigdom{\`e}nech and Mirza, Mehdi and Graves, Alex and Harley, Tim and Lillicrap, Timothy P and Silver, David and Kavukcuoglu, Koray},
  abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
  langid = {english},
  file = {/home/ming/snap/zotero-snap/common/Zotero/storage/GX6DGX5E/Mnih 等 - Asynchronous Methods for Deep Reinforcement Learni.pdf}
}

@inproceedings{mnihAsynchronousMethodsDeep2016,
  title = {Asynchronous Methods for Deep Reinforcement Learning},
  booktitle = {International Conference on Machine Learning},
  author = {Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  year = {2016},
  pages = {1928--1937},
  publisher = {{PMLR}},
  file = {/home/ming/snap/zotero-snap/common/Zotero/storage/A4BMS6CK/Mnih 等 - 2016 - Asynchronous methods for deep reinforcement learni.pdf;/home/ming/snap/zotero-snap/common/Zotero/storage/TGIUMTJZ/mniha16.html}
}

@article{mnihHumanlevelControlDeep2015,
  title = {Human-Level Control through Deep Reinforcement Learning},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg},
  year = {2015},
  journal = {nature},
  volume = {518},
  number = {7540},
  pages = {529--533},
  publisher = {{Nature Publishing Group}},
  file = {/home/ming/snap/zotero-snap/common/Zotero/storage/FEKX8FCF/Mnih 等 - 2015 - Human-level control through deep reinforcement lea.pdf;/home/ming/snap/zotero-snap/common/Zotero/storage/XL3KWVC3/nature14236.html}
}

@misc{mnihPlayingAtariDeep2013a,
  title = {Playing {{Atari}} with {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  year = {2013},
  month = dec,
  number = {arXiv:1312.5602},
  eprint = {arXiv:1312.5602},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1312.5602},
  urldate = {2023-02-14},
  abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/ming/snap/zotero-snap/common/Zotero/storage/4NK6JUZZ/Mnih 等 - 2013 - Playing Atari with Deep Reinforcement Learning.pdf;/home/ming/snap/zotero-snap/common/Zotero/storage/P9LLSZ48/1312.html}
}

@article{mullerOffroadObstacleAvoidance2005,
  title = {Off-Road Obstacle Avoidance through End-to-End Learning},
  author = {Muller, Urs and Ben, Jan and Cosatto, Eric and Flepp, Beat and Cun, Yann},
  year = {2005},
  journal = {Advances in neural information processing systems},
  volume = {18},
  file = {/home/ming/snap/zotero-snap/common/Zotero/storage/T42IL3QI/Muller 等 - 2005 - Off-road obstacle avoidance through end-to-end lea.pdf;/home/ming/snap/zotero-snap/common/Zotero/storage/PGGWSVF7/fdf1bc5669e8ff5ba45d02fded729feb-Abstract.html}
}

@article{OdinTeamVictorTango2008,
  title = {Odin: {{Team VictorTango}}'s Entry in the {{DARPA Urban Challenge}}},
  shorttitle = {Odin},
  year = {2008},
  journal = {Journal of Field Robotics},
  doi = {10.1002/rob.20248},
  urldate = {2023-01-10},
  abstract = {The DARPA Urban Challenge required robotic vehicles to travel more than 90 km through an urban environment without human intervention and included situations such as stop intersections, traffic merges, parking, and roadblocks. Team VictorTango separated the problem into three parts: base vehicle, perception, and planning. A Ford Escape outfitted with a custom drive-by-wire system and computers formed the basis for Odin. Perception used laser scanners, global positioning system, and a priori knowledge to identify obstacles, cars, and roads. Planning relied on a hybrid deliberative/reactive architecture to analyze the situation, select the appropriate behavior, and plan a safe path. All vehicle modules communicated using the JAUS (Joint Architecture for Unmanned Systems) standard. The performance of these components in the Urban Challenge is discussed and successes noted. The result of VictorTango's work was successful completion of the Urban Challenge and a third-place finish. \textcopyright{} 20},
  file = {/home/ming/snap/zotero-snap/common/Zotero/storage/MHPNUSDW/show.html}
}

@article{pivtoraikoDifferentiallyConstrainedMobile2009,
  title = {Differentially Constrained Mobile Robot Motion Planning in State Lattices},
  author = {Pivtoraiko, Mihail and Knepper, Ross A. and Kelly, Alonzo},
  year = {2009},
  journal = {Journal of Field Robotics},
  volume = {26},
  number = {3},
  pages = {308--333},
  publisher = {{Wiley Online Library}},
  doi = {10.1002/rob.20285},
  annotation = {titleTranslation: 状态格中的微分约束移动机器人运动规划},
  file = {/home/ming/snap/zotero-snap/common/Zotero/storage/XPLMVMMW/Pivtoraiko et al_2009_Differentially constrained mobile robot motion planning in state lattices.pdf;/home/ming/snap/zotero-snap/common/Zotero/storage/YKZJXKVV/rob.html}
}

@article{samuelReviewPurepursuitBased2016,
  title = {A Review of Some Pure-Pursuit Based Path Tracking Techniques for Control of Autonomous Vehicle},
  author = {Samuel, Moveh and Hussein, Mohamed and Mohamad, Maziah Binti},
  year = {2016},
  journal = {International Journal of Computer Applications},
  volume = {135},
  number = {1},
  pages = {35--38},
  publisher = {{Foundation of Computer Science}},
  doi = {10.5120/ijca2016908314},
  file = {/home/ming/snap/zotero-snap/common/Zotero/storage/GVHBB3A6/Samuel et al_2016_A review of some pure-pursuit based path tracking techniques for control of.pdf}
}

@article{schaulPrioritizedExperienceReplay2015,
  title = {Prioritized Experience Replay},
  author = {Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
  year = {2015},
  journal = {arXiv preprint arXiv:1511.05952},
  eprint = {1511.05952},
  archiveprefix = {arxiv},
  file = {/home/ming/snap/zotero-snap/common/Zotero/storage/VN9L2XL9/Schaul 等 - 2015 - Prioritized experience replay.pdf;/home/ming/snap/zotero-snap/common/Zotero/storage/NBQFYX9M/1511.html}
}

@inproceedings{shenReviewTrajectoryTracking2020,
  title = {A Review on Trajectory Tracking Control Algorithms for Nonholonomic Robots},
  booktitle = {2020 {{International Conference}} on {{Artificial Intelligence}} and {{Education}} ({{ICAIE}})},
  author = {Shen, Rui},
  year = {2020},
  month = jun,
  pages = {43--46},
  doi = {10.1109/ICAIE50891.2020.00018},
  abstract = {With the development of technology, the problem of trajectory tracking control for nonholonomic robots has great research value. This paper attempts to present a literature review on research results in trajectory tracking control for nonholonomic robots. First, this paper reviews the development history of trajectory tracking. Then we enumerate the trajectory tracking control algorithms. Finally, the summary and prospect of this paper are provided.},
  keywords = {Artificial intelligence,Education,kinematic model,nonholonomic robots,Predictive control,Robot kinematics,Sliding mode control,trajectory tracking control},
  file = {/home/ming/snap/zotero-snap/common/Zotero/storage/GI2VUPCY/Shen - 2020 - A review on trajectory tracking control algorithms.pdf;/home/ming/snap/zotero-snap/common/Zotero/storage/YZ4K9HZG/9262531.html}
}

@inproceedings{silver2014deterministic,
  title = {Deterministic Policy Gradient Algorithms},
  booktitle = {International Conference on Machine Learning},
  author = {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
  year = {2014},
  pages = {387--395},
  organization = {{Pmlr}},
  keywords = {⛔ No DOI found},
  file = {/home/ming/snap/zotero-snap/common/Zotero/storage/95ZPHUBS/Silver 等 - 2014 - Deterministic policy gradient algorithms.pdf;/home/ming/snap/zotero-snap/common/Zotero/storage/J8XPFNUK/silver14.html}
}

@inproceedings{silverDeterministicPolicyGradient2014a,
  title = {Deterministic Policy Gradient Algorithms},
  booktitle = {International Conference on Machine Learning},
  author = {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
  year = {2014},
  pages = {387--395},
  publisher = {{Pmlr}},
  keywords = {⛔ No DOI found},
  file = {/home/ming/snap/zotero-snap/common/Zotero/storage/IV5Q87B3/Silver et al_2014_Deterministic policy gradient algorithms.pdf;/home/ming/snap/zotero-snap/common/Zotero/storage/AGTLIFBD/silver14.html}
}

@article{sniderAutomaticSteeringMethods2009,
  title = {Automatic Steering Methods for Autonomous Automobile Path Tracking},
  author = {Snider, Jarrod M.},
  year = {2009},
  journal = {Robotics Institute, Pittsburgh, PA, Tech. Rep. CMU-RITR-09-08},
  file = {/home/ming/snap/zotero-snap/common/Zotero/storage/R794AFNN/Snider - 2009 - Automatic steering methods for autonomous automobi.pdf}
}

@article{spielbergNeuralNetworkVehicle2019,
  title = {Neural Network Vehicle Models for High-Performance Automated Driving},
  author = {Spielberg, Nathan A. and Brown, Matthew and Kapania, Nitin R. and Kegelman, John C. and Gerdes, J. Christian},
  year = {2019},
  journal = {Science robotics},
  volume = {4},
  number = {28},
  pages = {eaaw1975},
  publisher = {{American Association for the Advancement of Science}},
  file = {/home/ming/snap/zotero-snap/common/Zotero/storage/YL9JVPWM/scirobotics.html}
}

@book{sutton2018reinforcement,
  title = {Reinforcement Learning: {{An}} Introduction},
  author = {Sutton, Richard S and Barto, Andrew G},
  year = {2018},
  publisher = {{MIT press}}
}

@book{suttonReinforcementLearningSecond2018,
  title = {Reinforcement {{Learning}}, Second Edition: {{An Introduction}}},
  shorttitle = {Reinforcement {{Learning}}, Second Edition},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  year = {2018},
  month = nov,
  publisher = {{MIT Press}},
  abstract = {The significantly expanded and updated new edition of a widely used text on reinforcement learning, one of the most active research areas in artificial intelligence.Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms. This second edition has been significantly expanded and updated, presenting new topics and updating coverage of other topics.Like the first edition, this second edition focuses on core online learning algorithms, with the more mathematical material set off in shaded boxes. Part I covers as much of reinforcement learning as possible without going beyond the tabular case for which exact solutions can be found. Many algorithms presented in this part are new to the second edition, including UCB, Expected Sarsa, and Double Learning. Part II extends these ideas to function approximation, with new sections on such topics as artificial neural networks and the Fourier basis, and offers expanded treatment of off-policy learning and policy-gradient methods. Part III has new chapters on reinforcement learning's relationships to psychology and neuroscience, as well as an updated case-studies chapter including AlphaGo and AlphaGo Zero, Atari game playing, and IBM Watson's wagering strategy. The final chapter discusses the future societal impacts of reinforcement learning.},
  keywords = {Computers / Artificial Intelligence / General},
  annotation = {titleTranslation:}
}

@inproceedings{vanhasseltDeepReinforcementLearning2016,
  title = {Deep Reinforcement Learning with Double Q-Learning},
  booktitle = {Proceedings of the {{AAAI}} Conference on Artificial Intelligence},
  author = {Van Hasselt, Hado and Guez, Arthur and Silver, David},
  year = {2016},
  volume = {30},
  file = {/home/ming/snap/zotero-snap/common/Zotero/storage/JQUDBWXG/Van Hasselt 等 - 2016 - Deep reinforcement learning with double q-learning.pdf;/home/ming/snap/zotero-snap/common/Zotero/storage/MVJ2GVU7/10295.html}
}

@misc{vaswaniAttentionAllYou2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  month = dec,
  number = {arXiv:1706.03762},
  eprint = {arXiv:1706.03762},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1706.03762},
  urldate = {2023-02-17},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/ming/snap/zotero-snap/common/Zotero/storage/7DJT8GIK/Vaswani 等 - 2017 - Attention Is All You Need.pdf;/home/ming/snap/zotero-snap/common/Zotero/storage/3VC668CJ/1706.html}
}

@article{wachterImplementationInteriorPointFilter2006,
  title = {On the {{Implementation}} of an {{Interior-Point Filter Line-Search Algorithm}} for {{Large-Scale Nonlinear Programming}}},
  author = {W{\"a}chter, Andreas and Biegler, Lorenz},
  year = {2006},
  month = mar,
  journal = {Mathematical programming},
  volume = {106},
  pages = {25--57},
  doi = {10.1007/s10107-004-0559-y},
  abstract = {We present a primal-dual interior-point algorithm with a lter line-search method for non- linear programming. Local and global convergence properties of this method were analyzed in previous work. Here we provide a comprehensive description of the algorithm, including the fea- sibility restoration phase for the lter method, second-order corrections, and inertia correction of the KKT matrix. Heuristics are also considered that allow faster performance. This method has been implemented in the IPOPT code, which we demonstrate in a detailed numerical study based on 954 problems from the CUTEr test set. An evaluation is made of several line-search options, and a comparison is provided with two state-of-the-art interior-point codes for nonlin- ear programming.},
  file = {/home/ming/snap/zotero-snap/common/Zotero/storage/LTHEH6SU/Wächter 和 Biegler - 2006 - On the implementation of an interior-point filter .pdf}
}

@misc{wangDuelingNetworkArchitectures2016,
  title = {Dueling {{Network Architectures}} for {{Deep Reinforcement Learning}}},
  author = {Wang, Ziyu and Schaul, Tom and Hessel, Matteo and {van Hasselt}, Hado and Lanctot, Marc and {de Freitas}, Nando},
  year = {2016},
  month = apr,
  number = {arXiv:1511.06581},
  eprint = {arXiv:1511.06581},
  publisher = {{arXiv}},
  urldate = {2023-02-11},
  abstract = {In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/ming/snap/zotero-snap/common/Zotero/storage/3SNV7MSN/Wang 等 - 2016 - Dueling Network Architectures for Deep Reinforceme.pdf;/home/ming/snap/zotero-snap/common/Zotero/storage/B7SL8H5T/1511.html}
}

@article{xuReinforcementLearningMultiple2022,
  title = {Reinforcement {{Learning With Multiple Relational Attention}} for {{Solving Vehicle Routing Problems}}},
  author = {Xu, Yunqiu and Fang, Meng and Chen, Ling and Xu, Gangyan and Du, Yali and Zhang, Chengqi},
  year = {2022},
  month = oct,
  journal = {IEEE Transactions on Cybernetics},
  volume = {52},
  number = {10},
  pages = {11107--11120},
  issn = {2168-2275},
  doi = {10.1109/TCYB.2021.3089179},
  abstract = {In this article, we study the reinforcement learning (RL) for vehicle routing problems (VRPs). Recent works have shown that attention-based RL models outperform recurrent neural network-based methods on these problems in terms of both effectiveness and efficiency. However, existing RL models simply aggregate node embeddings to generate the context embedding without taking into account the dynamic network structures, making them incapable of modeling the state transition and action selection dynamics. In this work, we develop a new attention-based RL model that provides enhanced node embeddings via batch normalization reordering and gate aggregation, as well as dynamic-aware context embedding through an attentive aggregation module on multiple relational structures. We conduct experiments on five types of VRPs: 1) travelling salesman problem (TSP); 2) capacitated VRP (CVRP); 3) split delivery VRP (SDVRP); 4) orienteering problem (OP); and 5) prize collecting TSP (PCTSP). The results show that our model not only outperforms the learning-based baselines but also solves the problems much faster than the traditional baselines. In addition, our model shows improved generalizability when being evaluated in large-scale problems, as well as problems with different data distributions.},
  keywords = {Attention mechanism,combinatorial optimization,Computational modeling,Context modeling,Decoding,deep reinforcement learning (DRL),Optimization,Task analysis,traveling salesman problem,Vehicle dynamics,Vehicle routing,vehicle routing problem (VRP)},
  file = {/home/ming/snap/zotero-snap/common/Zotero/storage/J4Z7HMGG/Xu 等 - 2022 - Reinforcement Learning With Multiple Relational At.pdf;/home/ming/snap/zotero-snap/common/Zotero/storage/ZDVQD66B/9478307.html}
}

@misc{zhaoMUSEParallelMultiScale2019,
  title = {{{MUSE}}: {{Parallel Multi-Scale Attention}} for {{Sequence}} to {{Sequence Learning}}},
  shorttitle = {{{MUSE}}},
  author = {Zhao, Guangxiang and Sun, Xu and Xu, Jingjing and Zhang, Zhiyuan and Luo, Liangchen},
  year = {2019},
  month = nov,
  number = {arXiv:1911.09483},
  eprint = {arXiv:1911.09483},
  publisher = {{arXiv}},
  urldate = {2023-02-18},
  abstract = {In sequence to sequence learning, the self-attention mechanism proves to be highly effective, and achieves significant improvements in many tasks. However, the self-attention mechanism is not without its own flaws. Although self-attention can model extremely long dependencies, the attention in deep layers tends to overconcentrate on a single token, leading to insufficient use of local information and difficultly in representing long sequences. In this work, we explore parallel multi-scale representation learning on sequence data, striving to capture both long-range and short-range language structures. To this end, we propose the Parallel MUlti-Scale attEntion (MUSE) and MUSE-simple. MUSE-simple contains the basic idea of parallel multi-scale sequence representation learning, and it encodes the sequence in parallel, in terms of different scales with the help from self-attention, and pointwise transformation. MUSE builds on MUSE-simple and explores combining convolution and self-attention for learning sequence representations from more different scales. We focus on machine translation and the proposed approach achieves substantial performance improvements over Transformer, especially on long sequences. More importantly, we find that although conceptually simple, its success in practice requires intricate considerations, and the multi-scale attention must build on unified semantic space. Under common setting, the proposed model achieves substantial performance and outperforms all previous models on three main machine translation tasks. In addition, MUSE has potential for accelerating inference due to its parallelism. Code will be available at https://github.com/lancopku/MUSE},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/ming/snap/zotero-snap/common/Zotero/storage/ZXL3Q2MV/Zhao 等 - 2019 - MUSE Parallel Multi-Scale Attention for Sequence .pdf;/home/ming/snap/zotero-snap/common/Zotero/storage/Q8DUS7RA/1911.html}
}

@article{zhaoPathFollowingOptimization2021a,
  title = {Path {{Following Optimization}} for an {{Underactuated USV Using Smoothly-Convergent Deep Reinforcement Learning}}},
  author = {Zhao, Yujiao and Qi, Xin and Ma, Yong and Li, Zhixiong and Malekian, Reza and Sotelo, Miguel Angel},
  year = {2021},
  month = oct,
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  volume = {22},
  number = {10},
  pages = {6208--6220},
  issn = {1558-0016},
  doi = {10.1109/TITS.2020.2989352},
  abstract = {This paper aims to solve the path following problem for an underactuated unmanned-surface-vessel (USV) based on deep reinforcement learning (DRL). A smoothly-convergent DRL (SCDRL) method is proposed based on the deep Q network (DQN) and reinforcement learning. In this new method, an improved DQN structure was developed as a decision-making network to reduce the complexity of the control law for the path following of a three-degree of freedom USV model. An exploring function was proposed based on the adaptive gradient descent to extract the training knowledge for the DQN from the empirical data. In addition, a new reward function was designed to evaluate the output decisions of the DQN, and hence, to reinforce the decision-making network in controlling the USV path following. Numerical simulations were conducted to evaluate the performance of the proposed method. The analysis results demonstrate that the proposed SCDRL converges more smoothly than the traditional deep Q learning while the path following error of the SCDRL is comparable to existing methods. Thanks to good usability and generality of the proposed method for USV path following, it can be applied to practical applications.},
  keywords = {adaptive gradient descent,Decision making,deep reinforcement learning,Dynamics,Mathematical model,Optimization,path following,Reinforcement learning,Training,Underactuated USV,Vehicle dynamics},
  file = {/home/ming/snap/zotero-snap/common/Zotero/storage/AN7DG7AV/Zhao 等 - 2021 - Path Following Optimization for an Underactuated U.pdf;/home/ming/snap/zotero-snap/common/Zotero/storage/DQEYIJ6F/9086759.html}
}

@inproceedings{zhouLearningNavigateRough2022,
  title = {Learning to Navigate on the Rough Terrain: {{A}} Multi-Modal Deep Reinforcement Learning Approach},
  shorttitle = {Learning to Navigate on the Rough Terrain},
  booktitle = {2022 {{IEEE}} 4th {{International Conference}} on {{Power}}, {{Intelligent Computing}} and {{Systems}} ({{ICPICS}})},
  author = {Zhou, Bo and Yi, Jianjun and Zhang, Xinke},
  year = {2022},
  month = jul,
  pages = {189--194},
  doi = {10.1109/ICPICS55264.2022.9873725},
  abstract = {How to enable safe navigation of unmanned vehicles on complex and rough terrain is challenging and meaningful research. In this paper, we propose an end-to-end reinforcement learning local navigation method with multi-modal data fusion, which effectively combines the intrinsic perception, such as Inertial Measurement Unit (IMU) measurements, and the extrinsic perception, such as Three-dimensional (3D) point clouds and images, of an unmanned vehicle. A specific feature extraction network is constructed for each modal data, and the total network is effectively trained using a modal separation learning method. The experimental results show that the proposed method can effectively address various obstacles such as rough roads, vegetation obstacles, and water pool disturbances to achieve autonomous and safe navigation of unmanned vehicles in off-road scenarios.},
  keywords = {Autonomous Navigation,Deep Reinforcement Learning,Image segmentation,Measurement units,Multi-Modal Data Fusion,Navigation,Point cloud compression,Roads,Rough Terrain,Three-dimensional displays,Vegetation mapping},
  file = {/home/ming/snap/zotero-snap/common/Zotero/storage/JI6P49EE/Zhou et al_2022_Learning to navigate on the rough terrain.pdf;/home/ming/snap/zotero-snap/common/Zotero/storage/ZEK9ZGQ4/9873725.html}
}
